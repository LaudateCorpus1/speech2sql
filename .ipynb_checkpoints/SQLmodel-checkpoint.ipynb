{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dynet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6d10a6235822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdynet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dynet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import dynet as dy\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geography = pd.read_json('txt2sql-data/geography.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_variables(sql, sql_variables, sent, sent_variables):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    seen_sent_variables = set()\n",
    "    for token in sent.strip().split():\n",
    "        if (token not in sent_variables):\n",
    "            tokens.append(token)\n",
    "            tags.append(\"O\")\n",
    "        else:\n",
    "            assert len(sent_variables[token]) > 0\n",
    "            seen_sent_variables.add(token)\n",
    "            for word in sent_variables[token].split():\n",
    "                tokens.append(word)\n",
    "                tags.append(token)\n",
    "\n",
    "    sql_tokens = []\n",
    "    for token in sql.strip().split():\n",
    "        if token.startswith('\"%') or token.startswith(\"'%\"):\n",
    "            sql_tokens.append(token[:2])\n",
    "            token = token[2:]\n",
    "        elif token.startswith('\"') or token.startswith(\"'\"):\n",
    "            sql_tokens.append(token[0])\n",
    "            token = token[1:]\n",
    "\n",
    "        if token.endswith('%\"') or token.endswith(\"%'\"):\n",
    "            sql_tokens.append(token[:-2])\n",
    "            sql_tokens.append(token[-2:])\n",
    "        elif token.endswith('\"') or token.endswith(\"'\"):\n",
    "            sql_tokens.append(token[:-1])\n",
    "            sql_tokens.append(token[-1])\n",
    "        else:\n",
    "            sql_tokens.append(token)\n",
    "\n",
    "    template = []\n",
    "    complete = []\n",
    "    for token in sql_tokens:\n",
    "        # Do the template\n",
    "        if token in seen_sent_variables:\n",
    "            # The token is a variable name that will be copied from the sentence\n",
    "            template.append(token)\n",
    "        elif (token not in sent_variables) and (token not in sql_variables):\n",
    "            # The token is an SQL keyword\n",
    "            template.append(token)\n",
    "        elif token in sent_variables and sent_variables[token] != '':\n",
    "            # The token is a variable whose value is unique to this questions,\n",
    "            # but is not explicitly given\n",
    "            template.append(sent_variables[token])\n",
    "        else:\n",
    "            # The token is a variable whose value is not unique to this\n",
    "            # question and not explicitly given\n",
    "            template.append(sql_variables[token])\n",
    "\n",
    "        # Do the complete case\n",
    "        if token in sent_variables and sent_variables[token] != '':\n",
    "            complete.append(sent_variables[token])\n",
    "        elif token in sql_variables:\n",
    "            complete.append(sql_variables[token])\n",
    "        else:\n",
    "            complete.append(token)\n",
    "\n",
    "    return (tokens, tags, ' '.join(template), ' '.join(complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tagged_data_for_query(data):\n",
    "    \n",
    "\n",
    "    dataset = data['query-split']\n",
    "\n",
    "    for sent_info in data['sentences']:\n",
    "        for sql in data['sql']:\n",
    "            sql_vars = {}\n",
    "            for sql_var in data['variables']:\n",
    "                sql_vars[sql_var['name']] = sql_var['example']\n",
    "            text = sent_info['text']\n",
    "            text_vars = sent_info['variables']\n",
    "\n",
    "            yield (dataset, insert_variables(sql, sql_vars, text, text_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, w2i):\n",
    "        self.w2i = dict(w2i)\n",
    "        self.i2w = {i:w for w,i in w2i.items()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        w2i = {}\n",
    "        for word in corpus:\n",
    "            w2i.setdefault(word, len(w2i))\n",
    "        return Vocab(w2i)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.w2i.keys())\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    counts = Counter()\n",
    "    words = {\"<UNK>\"}\n",
    "    tag_set = set()\n",
    "    template_set = set()\n",
    "    for tokens, tags, template, complete in train:\n",
    "        template_set.add(template)\n",
    "        for tag in tags:\n",
    "            tag_set.add(tag)\n",
    "        for token in tokens:\n",
    "            counts[token] += 1\n",
    "\n",
    "    for word in counts:\n",
    "        if counts[word] > 0:\n",
    "            words.add(word)\n",
    "\n",
    "    vocab_tags = Vocab.from_corpus(tag_set)\n",
    "    vocab_words = Vocab.from_corpus(words)\n",
    "    vocab_templates = Vocab.from_corpus(template_set)\n",
    "\n",
    "    return vocab_words, vocab_tags, vocab_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "dev = []\n",
    "test = []\n",
    "with open('txt2sql-data/geography.json') as input_file:\n",
    "    data = json.load(input_file)\n",
    "    for example in data:\n",
    "        for dataset, instance in get_tagged_data_for_query(example):\n",
    "            if dataset == 'train':\n",
    "                train.append(instance)\n",
    "            elif dataset == 'dev':\n",
    "                train.append(instance)\n",
    "            elif dataset == 'test':\n",
    "                test.append(instance)\n",
    "            elif dataset == 'exclude':\n",
    "                pass\n",
    "            else:\n",
    "                assert False, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words, vocab_tags, vocab_templates = build_vocab(train)\n",
    "UNK = vocab_words.w2i[\"<UNK>\"]\n",
    "NWORDS = vocab_words.size()\n",
    "NTAGS = vocab_tags.size()\n",
    "NTEMPLATES = vocab_templates.size()\n",
    "\n",
    "print(f\"Running with {NTEMPLATES} templates\", f\"{NWORDS} words\", f\"{NTAGS} tags\", f\"{UNK} ignored words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dy.Model()\n",
    "trainer = dy.SimpleSGDTrainer(model, learning_rate=0.01)\n",
    "DIM_WORD = 128\n",
    "DIM_HIDDEN_LSTM = 64\n",
    "DIM_HIDDEN_MLP = 32\n",
    "DIM_HIDDEN_TEMPLATE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver = tf.train.import_meta_graph('advising-question-split-model/model.ckpt-35000.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     new_saver = tf.train.import_meta_graph('advising-question-split-model/model.ckpt-35000.meta')\n",
    "#     new_saver.restore(sess, tf.train.latest_checkpoint('advising-question-split-model/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build a Basic RNN\n",
    "\n",
    "# def enc_dec_model_inputs:\n",
    "#     return \n",
    "\n",
    "# def encoding_layer:\n",
    "#     return\n",
    "\n",
    "# def process_decoder_input:\n",
    "#     return\n",
    "\n",
    "# def decoding_layer_train:\n",
    "#     return\n",
    "\n",
    "# def decoding_layer_infer:\n",
    "#     return\n",
    "\n",
    "# def decoding_layer:\n",
    "#     return\n",
    "\n",
    "# def seq2seq_model:\n",
    "#     return\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
