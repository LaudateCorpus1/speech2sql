{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import dynet as dy\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "geography = pd.read_json('txt2sql-data/geography.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions and methods for preprocessing input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_variables(sql, sql_variables, sent, sent_variables):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    seen_sent_variables = set()\n",
    "    for token in sent.strip().split():\n",
    "        if (token not in sent_variables):\n",
    "            tokens.append(token)\n",
    "            tags.append(\"O\")\n",
    "        else:\n",
    "            assert len(sent_variables[token]) > 0\n",
    "            seen_sent_variables.add(token)\n",
    "            for word in sent_variables[token].split():\n",
    "                tokens.append(word)\n",
    "                tags.append(token)\n",
    "\n",
    "    sql_tokens = []\n",
    "    for token in sql.strip().split():\n",
    "        if token.startswith('\"%') or token.startswith(\"'%\"):\n",
    "            sql_tokens.append(token[:2])\n",
    "            token = token[2:]\n",
    "        elif token.startswith('\"') or token.startswith(\"'\"):\n",
    "            sql_tokens.append(token[0])\n",
    "            token = token[1:]\n",
    "\n",
    "        if token.endswith('%\"') or token.endswith(\"%'\"):\n",
    "            sql_tokens.append(token[:-2])\n",
    "            sql_tokens.append(token[-2:])\n",
    "        elif token.endswith('\"') or token.endswith(\"'\"):\n",
    "            sql_tokens.append(token[:-1])\n",
    "            sql_tokens.append(token[-1])\n",
    "        else:\n",
    "            sql_tokens.append(token)\n",
    "\n",
    "    template = []\n",
    "    complete = []\n",
    "    for token in sql_tokens:\n",
    "        # Do the template\n",
    "        if token in seen_sent_variables:\n",
    "            # The token is a variable name that will be copied from the sentence\n",
    "            template.append(token)\n",
    "        elif (token not in sent_variables) and (token not in sql_variables):\n",
    "            # The token is an SQL keyword\n",
    "            template.append(token)\n",
    "        elif token in sent_variables and sent_variables[token] != '':\n",
    "            # The token is a variable whose value is unique to this questions,\n",
    "            # but is not explicitly given\n",
    "            template.append(sent_variables[token])\n",
    "        else:\n",
    "            # The token is a variable whose value is not unique to this\n",
    "            # question and not explicitly given\n",
    "            template.append(sql_variables[token])\n",
    "\n",
    "        # Do the complete case\n",
    "        if token in sent_variables and sent_variables[token] != '':\n",
    "            complete.append(sent_variables[token])\n",
    "        elif token in sql_variables:\n",
    "            complete.append(sql_variables[token])\n",
    "        else:\n",
    "            complete.append(token)\n",
    "\n",
    "    return (tokens, tags, ' '.join(template), ' '.join(complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tagged_data_for_query(data):\n",
    "    \n",
    "    dataset = data['query-split']\n",
    "\n",
    "    for sent_info in data['sentences']:\n",
    "        for sql in data['sql']:\n",
    "            sql_vars = {}\n",
    "            for sql_var in data['variables']:\n",
    "                sql_vars[sql_var['name']] = sql_var['example']\n",
    "            text = sent_info['text']\n",
    "            text_vars = sent_info['variables']\n",
    "\n",
    "            yield (dataset, insert_variables(sql, sql_vars, text, text_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain class and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, w2i):\n",
    "        self.w2i = dict(w2i)\n",
    "        self.i2w = {i:w for w,i in w2i.items()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        w2i = {}\n",
    "        for word in corpus:\n",
    "            w2i.setdefault(word, len(w2i))\n",
    "        return Vocab(w2i)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.w2i.keys())\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    counts = Counter()\n",
    "    words = {\"<UNK>\"}\n",
    "    tag_set = set()\n",
    "    template_set = set()\n",
    "    for tokens, tags, template, complete in train:\n",
    "        template_set.add(template)\n",
    "        for tag in tags:\n",
    "            tag_set.add(tag)\n",
    "        for token in tokens:\n",
    "            counts[token] += 1\n",
    "\n",
    "    for word in counts:\n",
    "        if counts[word] > 0:\n",
    "            words.add(word)\n",
    "\n",
    "    vocab_tags = Vocab.from_corpus(tag_set)\n",
    "    vocab_words = Vocab.from_corpus(words)\n",
    "    vocab_templates = Vocab.from_corpus(template_set)\n",
    "\n",
    "    return vocab_words, vocab_tags, vocab_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "dev = []\n",
    "test = []\n",
    "with open('txt2sql-data/geography.json') as input_file:\n",
    "    data = json.load(input_file)\n",
    "    for example in data:\n",
    "        for dataset, instance in get_tagged_data_for_query(example):\n",
    "            if dataset == 'train':\n",
    "                train.append(instance)\n",
    "            elif dataset == 'dev':\n",
    "                train.append(instance)\n",
    "            elif dataset == 'test':\n",
    "                test.append(instance)\n",
    "            elif dataset == 'exclude':\n",
    "                pass\n",
    "            else:\n",
    "                assert False, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with 207 templates 267 words 3 tags 113 ignored words\n"
     ]
    }
   ],
   "source": [
    "vocab_words, vocab_tags, vocab_templates = build_vocab(train)\n",
    "UNK = vocab_words.w2i[\"<UNK>\"]\n",
    "NWORDS = vocab_words.size()\n",
    "NTAGS = vocab_tags.size()\n",
    "NTEMPLATES = vocab_templates.size()\n",
    "\n",
    "print(f\"Running with {NTEMPLATES} templates\", f\"{NWORDS} words\", f\"{NTAGS} tags\", f\"{UNK} ignored words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the model\n",
    "# [Enter description]\n",
    "# Dropout is a regularization method that prevents activation and weight updates for some input and recurrent connections to LSTM units during training.\n",
    "# This could help reducing overfitting and improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dy.Model()\n",
    "\n",
    "# This trainer performs stochastic gradient descent\n",
    "# this is the most common optimization procedure for neural networks.\n",
    "trainer = dy.SimpleSGDTrainer(model, learning_rate=0.01)\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "# Input dimension \n",
    "DIM_WORD = 128\n",
    "\n",
    "# Number of input layers\n",
    "LSTM_LAYERS = 2\n",
    "\n",
    "# Dimension of the recurrent units\n",
    "DIM_HIDDEN_LSTM = 64\n",
    "\n",
    "# Dimension of the hidden layers\n",
    "DIM_HIDDEN_MLP = 32\n",
    "\n",
    "# Dimension of the hidden layers that predict the template\n",
    "DIM_HIDDEN_TEMPLATE = 64\n",
    "\n",
    "# Word Embeddings\n",
    "pEmbedding = model.add_lookup_parameters((NWORDS, DIM_WORD))\n",
    "\n",
    "# Dropout rate\n",
    "DROPOUT = 0.1\n",
    "\n",
    "\n",
    "# Layers  \n",
    "pHidden = model.add_parameters((DIM_HIDDEN_MLP, DIM_HIDDEN_LSTM*2))\n",
    "pOutput = model.add_parameters((NTAGS, DIM_HIDDEN_MLP))\n",
    "\n",
    "# This allows us to create a standard LSTM\n",
    "builders = [\n",
    "    dy.LSTMBuilder(LSTM_LAYERS, DIM_WORD, DIM_HIDDEN_LSTM, model),\n",
    "    dy.LSTMBuilder(LSTM_LAYERS, DIM_WORD, DIM_HIDDEN_LSTM, model),\n",
    "]\n",
    "\n",
    "pHiddenTemplate = model.add_parameters((DIM_HIDDEN_TEMPLATE, DIM_HIDDEN_LSTM*2))\n",
    "pOutputTemplate = model.add_parameters((NTEMPLATES, DIM_HIDDEN_TEMPLATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the model\n",
    "\n",
    "def build_tagging_graph(words, tags, template, builders, train=True):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    if DROPOUT > 0:\n",
    "    # Sets the dropout rate\n",
    "        for b in builders:\n",
    "            b.set_dropouts(DROPOUT, DROPOUT)\n",
    "            \n",
    "    # Initialize bi-directional LSTM \n",
    "    f_init, b_init = [b.initial_state() for b in builders]\n",
    "    \n",
    "    wembs = [dy.lookup(pEmbedding, w) for w in words]\n",
    "    \n",
    "    # This adds noise to the training data as a regularizer\n",
    "    if train: \n",
    "        wembs = [dy.noise(we, args.train_noise) for we in wembs]\n",
    "    \n",
    "\n",
    "    fw_states = [x for x in f_init.add_inputs(wembs)]\n",
    "    fw = [x.output() for x in fw_states]\n",
    "    \n",
    "    bw_states = [x for x in b_init.add_inputs(reversed(wembs))]\n",
    "    bw = [x.output() for x in bw_states]    \n",
    "    \n",
    "    O = dy.parameter(pOutput)\n",
    "    H = dy.parameter(pHidden)\n",
    "    \n",
    "    #Errors\n",
    "    errs = []\n",
    "    #Predicted tags\n",
    "    pred_tags = []\n",
    "    \n",
    "    \n",
    "    for f, b, t in zip(fw, reversed(bw), tags):\n",
    "        f_b = dy.concatenate([f,b])\n",
    "        # Hyperbolic tangent activation function\n",
    "        f_b = dy.tanh(H * f_b)\n",
    "        r_t = O * f_b\n",
    "        \n",
    "        if train:\n",
    "            err = dy.pickneglogsoftmax(r_t, t)\n",
    "            errs.append(err)\n",
    "        else:\n",
    "            out = dy.softmax(r_t)\n",
    "            chosen = np.argmax(out.npvalue())\n",
    "            pred_tags.append(vocab_tags.i2w[chosen])\n",
    "            \n",
    "        return pred_tags, pred_template, errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_tagged_tokens(tokens, tags, template):\n",
    "    to_insert = {}\n",
    "    cur = (None, [])\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag != cur[0]:\n",
    "            if cur[0] is not None:\n",
    "                value = ' '.join(cur[1])\n",
    "                to_insert[cur[0]] = value\n",
    "            if tag == 'O':\n",
    "                cur = (None, [])\n",
    "            else:\n",
    "                cur = (tag, [token])\n",
    "        else:\n",
    "            cur[1].append(token)\n",
    "    if cur[0] is not None:\n",
    "        value = ' '.join(cur[1])\n",
    "        to_insert[cur[0]] = value\n",
    "\n",
    "    modified = []\n",
    "    for token in template.split():\n",
    "        modified.append(to_insert.get(token, token))\n",
    "\n",
    "    return ' '.join(modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(data, builders, iteration, step):\n",
    "    if len(data) == 0:\n",
    "        print(\"No data for eval\")\n",
    "        return -1\n",
    "    correct_tags = 0.0\n",
    "    total_tags = 0.0\n",
    "    complete_match = 0.0\n",
    "    templates_match = 0.0\n",
    "    oracle = 0.0\n",
    "    for tokens, tags, template, complete in data:\n",
    "        word_ids = [vocab_words.w2i.get(word, UNK) for word in tokens]\n",
    "        tag_ids = [0 for tag in tags]\n",
    "        pred_tags, pred_template, _ = build_tagging_graph(word_ids, tag_ids, 0, builders, False)\n",
    "        gold_tags = tags\n",
    "        for gold, pred in zip(gold_tags, pred_tags):\n",
    "            total_tags += 1\n",
    "            if gold == pred: correct_tags += 1\n",
    "        pred_complete = insert_tagged_tokens(tokens, pred_tags, pred_template)\n",
    "        if pred_complete == complete:\n",
    "            complete_match += 1\n",
    "        if pred_template == template:\n",
    "            templates_match += 1\n",
    "        if template in vocab_templates.w2i:\n",
    "            oracle += 1\n",
    "\n",
    "    tok_acc = correct_tags / total_tags\n",
    "    complete_acc = complete_match / len(data)\n",
    "    template_acc = templates_match / len(data)\n",
    "    oracle_acc = oracle / len(data)\n",
    "    \n",
    "    print(f\"Eval {iteration} - {step} Tag Acc: {tok_acc} Template: {template_acc} Complete: {complete_acc}\")\n",
    "    return complete_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
