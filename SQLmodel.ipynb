{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import dynet as dy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-split</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sql</th>\n",
       "      <th>variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>[{'question-split': 'dev', 'text': 'what is th...</td>\n",
       "      <td>[SELECT CITYalias0.CITY_NAME FROM CITY AS CITY...</td>\n",
       "      <td>[{'example': 'arizona', 'location': 'both', 'n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>[{'question-split': 'dev', 'text': 'which rive...</td>\n",
       "      <td>[SELECT RIVERalias0.RIVER_NAME FROM RIVER AS R...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dev</td>\n",
       "      <td>[{'question-split': 'dev', 'text': 'how big is...</td>\n",
       "      <td>[SELECT STATEalias0.AREA FROM STATE AS STATEal...</td>\n",
       "      <td>[{'example': 'texas', 'location': 'both', 'nam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>[{'question-split': 'dev', 'text': 'how many p...</td>\n",
       "      <td>[SELECT STATEalias0.POPULATION FROM STATE AS S...</td>\n",
       "      <td>[{'example': 'washington', 'location': 'both',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>[{'question-split': 'dev', 'text': 'what state...</td>\n",
       "      <td>[SELECT STATEalias0.STATE_NAME FROM STATE AS S...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query-split                                          sentences  \\\n",
       "0       train  [{'question-split': 'dev', 'text': 'what is th...   \n",
       "1        test  [{'question-split': 'dev', 'text': 'which rive...   \n",
       "2         dev  [{'question-split': 'dev', 'text': 'how big is...   \n",
       "3        test  [{'question-split': 'dev', 'text': 'how many p...   \n",
       "4       train  [{'question-split': 'dev', 'text': 'what state...   \n",
       "\n",
       "                                                 sql  \\\n",
       "0  [SELECT CITYalias0.CITY_NAME FROM CITY AS CITY...   \n",
       "1  [SELECT RIVERalias0.RIVER_NAME FROM RIVER AS R...   \n",
       "2  [SELECT STATEalias0.AREA FROM STATE AS STATEal...   \n",
       "3  [SELECT STATEalias0.POPULATION FROM STATE AS S...   \n",
       "4  [SELECT STATEalias0.STATE_NAME FROM STATE AS S...   \n",
       "\n",
       "                                           variables  \n",
       "0  [{'example': 'arizona', 'location': 'both', 'n...  \n",
       "1                                                 []  \n",
       "2  [{'example': 'texas', 'location': 'both', 'nam...  \n",
       "3  [{'example': 'washington', 'location': 'both',...  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geography = pd.read_json('txt2sql-data/geography.json')\n",
    "geography.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions and methods for preprocessing input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_variables(sql, sql_variables, sent, sent_variables):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    seen_sent_variables = set()\n",
    "    for token in sent.strip().split():\n",
    "        if (token not in sent_variables):\n",
    "            tokens.append(token)\n",
    "            tags.append(\"O\")\n",
    "        else:\n",
    "            assert len(sent_variables[token]) > 0\n",
    "            seen_sent_variables.add(token)\n",
    "            for word in sent_variables[token].split():\n",
    "                tokens.append(word)\n",
    "                tags.append(token)\n",
    "\n",
    "    sql_tokens = []\n",
    "    for token in sql.strip().split():\n",
    "        if token.startswith('\"%') or token.startswith(\"'%\"):\n",
    "            sql_tokens.append(token[:2])\n",
    "            token = token[2:]\n",
    "        elif token.startswith('\"') or token.startswith(\"'\"):\n",
    "            sql_tokens.append(token[0])\n",
    "            token = token[1:]\n",
    "\n",
    "        if token.endswith('%\"') or token.endswith(\"%'\"):\n",
    "            sql_tokens.append(token[:-2])\n",
    "            sql_tokens.append(token[-2:])\n",
    "        elif token.endswith('\"') or token.endswith(\"'\"):\n",
    "            sql_tokens.append(token[:-1])\n",
    "            sql_tokens.append(token[-1])\n",
    "        else:\n",
    "            sql_tokens.append(token)\n",
    "\n",
    "    template = []\n",
    "    complete = []\n",
    "    for token in sql_tokens:\n",
    "        # Do the template\n",
    "        if token in seen_sent_variables:\n",
    "            # The token is a variable name that will be copied from the sentence\n",
    "            template.append(token)\n",
    "        elif (token not in sent_variables) and (token not in sql_variables):\n",
    "            # The token is an SQL keyword\n",
    "            template.append(token)\n",
    "        elif token in sent_variables and sent_variables[token] != '':\n",
    "            # The token is a variable whose value is unique to this questions,\n",
    "            # but is not explicitly given\n",
    "            template.append(sent_variables[token])\n",
    "        else:\n",
    "            # The token is a variable whose value is not unique to this\n",
    "            # question and not explicitly given\n",
    "            template.append(sql_variables[token])\n",
    "\n",
    "        # Do the complete case\n",
    "        if token in sent_variables and sent_variables[token] != '':\n",
    "            complete.append(sent_variables[token])\n",
    "        elif token in sql_variables:\n",
    "            complete.append(sql_variables[token])\n",
    "        else:\n",
    "            complete.append(token)\n",
    "\n",
    "    return (tokens, tags, ' '.join(template), ' '.join(complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_tagged_data_for_query(data):\n",
    "    \n",
    "#     dataset = data['query-split']\n",
    "\n",
    "#     for sent_info in data['sentences']:\n",
    "#         for sql in data['sql']:\n",
    "#             sql_vars = {}\n",
    "#             for sql_var in data['variables']:\n",
    "#                 sql_vars[sql_var['name']] = sql_var['example']\n",
    "#             text = sent_info['text']\n",
    "#             text_vars = sent_info['variables']\n",
    "\n",
    "#             yield (dataset, insert_variables(sql, sql_vars, text, text_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tagged_data_for_query(data):\n",
    "    # By default, set to the query split value\n",
    "    dataset = data['query-split']\n",
    "    # split can be adjusted for small datasets that require cross-validation\n",
    "    split = None\n",
    "    use_all_sql = True\n",
    "    \n",
    "    if split is not None:\n",
    "        if str(split) == str(dataset):\n",
    "            dataset = \"test\"\n",
    "        else:\n",
    "            dataset = \"train\"\n",
    "\n",
    "    for sent_info in data['sentences']:\n",
    "        # For question split, set to this question's value\n",
    "        dataset = sent_info['question-split']\n",
    "        if split is not None:\n",
    "            if str(split) == str(dataset):\n",
    "                dataset = \"test\"\n",
    "            else:\n",
    "                dataset = \"train\"\n",
    "\n",
    "        for sql in data['sql']:\n",
    "            sql_vars = {}\n",
    "            for sql_var in data['variables']:\n",
    "                sql_vars[sql_var['name']] = sql_var['example']\n",
    "            text = sent_info['text']\n",
    "            text_vars = sent_info['variables']\n",
    "\n",
    "            yield (dataset, insert_variables(sql, sql_vars, text, text_vars))\n",
    "\n",
    "            if not use_all_sql:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, w2i):\n",
    "        self.w2i = dict(w2i)\n",
    "        self.i2w = {i:w for w,i in w2i.items()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        w2i = {}\n",
    "        for word in corpus:\n",
    "            w2i.setdefault(word, len(w2i))\n",
    "        return Vocab(w2i)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.w2i.keys())\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    counts = Counter()\n",
    "    words = {\"<UNK>\"}\n",
    "    tag_set = set()\n",
    "    template_set = set()\n",
    "    for tokens, tags, template, complete in train:\n",
    "        template_set.add(template)\n",
    "        for tag in tags:\n",
    "            tag_set.add(tag)\n",
    "        for token in tokens:\n",
    "            counts[token] += 1\n",
    "\n",
    "    for word in counts:\n",
    "        if counts[word] > 0:\n",
    "            words.add(word)\n",
    "\n",
    "    vocab_tags = Vocab.from_corpus(tag_set)\n",
    "    vocab_words = Vocab.from_corpus(words)\n",
    "    vocab_templates = Vocab.from_corpus(template_set)\n",
    "\n",
    "    return vocab_words, vocab_tags, vocab_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "dev = []\n",
    "test = []\n",
    "with open('txt2sql-data/advising.json') as input_file:\n",
    "    data = json.load(input_file)\n",
    "    for example in data:\n",
    "        for dataset, instance in get_tagged_data_for_query(example):\n",
    "            if dataset == 'train':\n",
    "                train.append(instance)\n",
    "            elif dataset == 'dev':\n",
    "                train.append(instance)\n",
    "            elif dataset == 'test':\n",
    "                test.append(instance)\n",
    "            elif dataset == 'exclude':\n",
    "                pass\n",
    "            else:\n",
    "                assert False, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can', 'undergrads', 'take', '550', '?']\n",
      "['O', 'O', 'O', 'number0', 'O']\n",
      "SELECT DISTINCT COURSEalias0.ADVISORY_REQUIREMENT , COURSEalias0.ENFORCED_REQUIREMENT , COURSEalias0.NAME FROM COURSE AS COURSEalias0 WHERE COURSEalias0.DEPARTMENT = \" EECS \" AND COURSEalias0.NUMBER = number0 ;\n",
      "SELECT DISTINCT COURSEalias0.ADVISORY_REQUIREMENT , COURSEalias0.ENFORCED_REQUIREMENT , COURSEalias0.NAME FROM COURSE AS COURSEalias0 WHERE COURSEalias0.DEPARTMENT = \" EECS \" AND COURSEalias0.NUMBER = 550 ;\n"
     ]
    }
   ],
   "source": [
    "for tokens, tags, template, complete in train:\n",
    "    print(tokens)\n",
    "    print(tags)\n",
    "    print(template)\n",
    "    print(complete)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with 219 templates 2591 words 23 tags 29 unknown words\n"
     ]
    }
   ],
   "source": [
    "vocab_words, vocab_tags, vocab_templates = build_vocab(train)\n",
    "UNK = vocab_words.w2i[\"<UNK>\"]\n",
    "NWORDS = vocab_words.size()\n",
    "NTAGS = vocab_tags.size()\n",
    "NTEMPLATES = vocab_templates.size()\n",
    "\n",
    "print(f\"Running with {NTEMPLATES} templates\", f\"{NWORDS} words\", f\"{NTAGS} tags\", f\"{UNK} unknown words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "Recurrent neural networks (RNNs) are designed to solve sequence modelling problems like translations, because they process information incrementally and can maintain an internal state that uses past information to inform predictions.\n",
    "\n",
    "A Long Short-Term Memory (LSTM) network is a type of recurrent neural network that has LSTM cell blocks instead of typical neural network layers. The main components of these cells are the input gate, the forget gate and the output gate. This architecture solves the <a href=\"https://en.wikipedia.org/wiki/Vanishing_gradient_problem\">vanishing gradient problem</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/LSTM.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/\">Source</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dy.Model()\n",
    "\n",
    "# This trainer performs stochastic gradient descent\n",
    "# this is the most common optimization procedure for neural networks.\n",
    "trainer = dy.SimpleSGDTrainer(model, learning_rate=0.1)\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "# Input dimension \n",
    "DIM_WORD = 64\n",
    "\n",
    "# Number of input layers\n",
    "LSTM_LAYERS = 2\n",
    "\n",
    "# Dimension of the recurrent units\n",
    "DIM_HIDDEN_LSTM = 128\n",
    "\n",
    "# Dimension of the hidden layers\n",
    "DIM_HIDDEN_MLP = 32\n",
    "\n",
    "# Dimension of the hidden layers that predict the template\n",
    "DIM_HIDDEN_TEMPLATE = 32\n",
    "\n",
    "# Word Embeddings\n",
    "pEmbedding = model.add_lookup_parameters((NWORDS, DIM_WORD))\n",
    "\n",
    "# Layers  \n",
    "pHidden = model.add_parameters((DIM_HIDDEN_MLP, DIM_HIDDEN_LSTM*2))\n",
    "pOutput = model.add_parameters((NTAGS, DIM_HIDDEN_MLP))\n",
    "\n",
    "# This allows us to create a standard LSTM\n",
    "builders = [\n",
    "    dy.LSTMBuilder(LSTM_LAYERS, DIM_WORD, DIM_HIDDEN_LSTM, model),\n",
    "    dy.LSTMBuilder(LSTM_LAYERS, DIM_WORD, DIM_HIDDEN_LSTM, model),\n",
    "]\n",
    "\n",
    "pHiddenTemplate = model.add_parameters((DIM_HIDDEN_TEMPLATE, DIM_HIDDEN_LSTM*2))\n",
    "pOutputTemplate = model.add_parameters((NTEMPLATES, DIM_HIDDEN_TEMPLATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(words, tags, template, builders, train=True):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    '''\n",
    "    Dropout is a regularization method that prevents activation and weight updates \n",
    "    for some input and recurrent connections to LSTM units during training.\n",
    "    This could help reducing overfitting and improve the model's performance.\n",
    "    '''\n",
    "    \n",
    "    dropout_rate = 0\n",
    "    mlp = True\n",
    "\n",
    "    if train and dropout_rate is not None and dropout_rate > 0:\n",
    "        for b in builders:\n",
    "            b.set_dropouts(dropout_rate, dropout_rate)\n",
    "\n",
    "    f_init, b_init = [b.initial_state() for b in builders]\n",
    "\n",
    "    wembs = [dy.lookup(pEmbedding, w) for w in words]\n",
    "    if train: # Add noise in training as a regularizer\n",
    "        wembs = [dy.noise(we, 0.1) for we in wembs]\n",
    "\n",
    "    fw_states = [x for x in f_init.add_inputs(wembs)]\n",
    "    bw_states = [x for x in b_init.add_inputs(reversed(wembs))]\n",
    "    fw = [x.output() for x in fw_states]\n",
    "    bw = [x.output() for x in bw_states]\n",
    "    \n",
    "    # Output of the input gate\n",
    "    O = dy.parameter(pOutput)\n",
    "    \n",
    "    if mlp:\n",
    "        # Output of the hidden layer\n",
    "        H = dy.parameter(pHidden)\n",
    "    errs = []\n",
    "    pred_tags = []\n",
    "    \n",
    "    for f, b, t in zip(fw, reversed(bw), tags):\n",
    "        # The current input, and output from the previous state are combined\n",
    "        f_b = dy.concatenate([f,b])\n",
    "        if mlp:\n",
    "        # The previous LSTM cell outputs are squashed b/n -1 and 1\n",
    "            f_b = dy.tanh(H * f_b)\n",
    "        # squashed input is then multiplied element-wise by the output of the input gate. \n",
    "        r_t = O * f_b\n",
    "        if train:\n",
    "            err = dy.pickneglogsoftmax(r_t, t)\n",
    "            errs.append(err)\n",
    "        else:\n",
    "            out = dy.softmax(r_t)\n",
    "            chosen = np.argmax(out.npvalue())\n",
    "            pred_tags.append(vocab_tags.i2w[chosen])\n",
    "\n",
    "    O_template = dy.parameter(pOutputTemplate)\n",
    "    H_template = dy.parameter(pHiddenTemplate)\n",
    "    f_bt = dy.concatenate([fw_states[-1].s()[0], bw_states[-1].s()[0]])\n",
    "    f_bt = dy.tanh(H_template * f_bt)\n",
    "    r_tt = O_template * f_bt\n",
    "    pred_template = None\n",
    "    if train:\n",
    "        err = dy.pickneglogsoftmax(r_tt, template)\n",
    "        errs.append(err)\n",
    "    else:\n",
    "        out = dy.softmax(r_tt)\n",
    "        chosen = np.argmax(out.npvalue())\n",
    "        pred_template = vocab_templates.i2w[chosen]\n",
    "\n",
    "    return pred_tags, pred_template, errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_tagged_tokens(tokens, tags, template):\n",
    "    to_insert = {}\n",
    "    cur = (None, [])\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag != cur[0]:\n",
    "            if cur[0] is not None:\n",
    "                value = ' '.join(cur[1])\n",
    "                to_insert[cur[0]] = value\n",
    "            if tag == 'O':\n",
    "                cur = (None, [])\n",
    "            else:\n",
    "                cur = (tag, [token])\n",
    "        else:\n",
    "            cur[1].append(token)\n",
    "    if cur[0] is not None:\n",
    "        value = ' '.join(cur[1])\n",
    "        to_insert[cur[0]] = value\n",
    "\n",
    "    modified = []\n",
    "    for token in template.split():\n",
    "        modified.append(to_insert.get(token, token))\n",
    "\n",
    "    return ' '.join(modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(data, builders, iteration, step):\n",
    "    if len(data) == 0:\n",
    "        print(\"No data for eval\")\n",
    "        return -1\n",
    "    correct_tags = 0.0\n",
    "    total_tags = 0.0\n",
    "    complete_match = 0.0\n",
    "    templates_match = 0.0\n",
    "    oracle = 0.0\n",
    "    for tokens, tags, template, complete in data:\n",
    "        word_ids = [vocab_words.w2i.get(word, UNK) for word in tokens]\n",
    "        tag_ids = [0 for tag in tags]\n",
    "        pred_tags, pred_template, _ = build_tagging_graph(word_ids, tag_ids, 0, builders, False)\n",
    "        gold_tags = tags\n",
    "        for gold, pred in zip(gold_tags, pred_tags):\n",
    "            total_tags += 1\n",
    "            if gold == pred: correct_tags += 1\n",
    "        pred_complete = insert_tagged_tokens(tokens, pred_tags, pred_template)\n",
    "        if pred_complete == complete:\n",
    "            complete_match += 1\n",
    "        if pred_template == template:\n",
    "            templates_match += 1\n",
    "        if template in vocab_templates.w2i:\n",
    "            oracle += 1\n",
    "\n",
    "    tok_acc = correct_tags / total_tags\n",
    "    complete_acc = complete_match / len(data)\n",
    "    template_acc = templates_match / len(data)\n",
    "    oracle_acc = oracle / len(data)\n",
    "    \n",
    "    print(f\"Eval {iteration} - {step} Tag Acc: {tok_acc} Template: {template_acc} Complete: {complete_acc}\")\n",
    "    return complete_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dy.parameter(...) call is now DEPRECATED.\n",
      "        There is no longer need to explicitly add parameters to the computation graph.\n",
      "        Any used parameter will be added automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [16:33<00:00, 19.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval End - test Tag Acc: 0.9874130016289057 Template: 0.7692307692307693 Complete: 0.7250409165302782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7250409165302782"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = 0\n",
    "loss = 0\n",
    "best_dev_acc = 0.0\n",
    "\n",
    "# Number of examples to decode between logging\n",
    "log_freq = 1000000\n",
    "# Number of examples to decode between evaluation runs\n",
    "eval_freq = 1000000\n",
    "\n",
    "iters_since_best_updated = 0\n",
    "iters_without_improvement = 5\n",
    "steps = 0\n",
    "iters = 50\n",
    "\n",
    "#  Training in 50 epochs'\n",
    "for iteration in tqdm(range(iters)):\n",
    "    random.shuffle(train)\n",
    "    for tokens, tags, template, complete in train:\n",
    "        steps += 1\n",
    "\n",
    "        # Convert to indices\n",
    "        word_ids = [vocab_words.w2i.get(word, UNK) for word in tokens]\n",
    "        tag_ids = [vocab_tags.w2i[tag] for tag in tags]\n",
    "        template_id = vocab_templates.w2i[template]\n",
    "\n",
    "        # Decode and update\n",
    "        _, _, errs = build_tagging_graph(word_ids, tag_ids, template_id, builders)\n",
    "        sum_errs = dy.esum(errs)\n",
    "        loss += sum_errs.scalar_value()\n",
    "        tagged += len(tag_ids)\n",
    "        sum_errs.backward()\n",
    "        trainer.update()\n",
    "\n",
    "        # Log status\n",
    "        if steps % log_freq == 0:\n",
    "            trainer.status()\n",
    "            print(\"TrainLoss {}-{}: {}\".format(iteration, steps, loss / tagged))\n",
    "            loss = 0\n",
    "            tagged = 0\n",
    "            sys.stdout.flush()\n",
    "        if steps % eval_freq == 0:\n",
    "            acc = run_eval(dev, builders, iteration, steps)\n",
    "            if best_dev_acc < acc:\n",
    "                best_dev_acc = acc\n",
    "                iters_since_best_updated = 0\n",
    "                print(\"New best Acc!\", acc)\n",
    "            sys.stdout.flush()\n",
    "    iters_since_best_updated += 1\n",
    "    if iters > 0 and iters_since_best_updated > 50:\n",
    "       print(f'Stopping at {steps} iterations as there have been {iters_without_improvement} iterations without improvement')\n",
    "       break\n",
    "        \n",
    "run_eval(test, builders, \"End\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges & Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "discuss how you would improve this in the future\n",
    "### Preprocessing\n",
    "Word embeddings are dense, floating-point vectors that are a powerful way encoding vocabulary. Word embeddings are used to map human language into a geometric space, so that geometric relationships between word vectors reflect the semantic relationships between words.\n",
    "\n",
    "### Building the model\n",
    "\n",
    "Mechanisms to improve model performance: Attention, beam search, copying.\n",
    "\n",
    "### Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@InProceedings{data-sql-advising,\n",
    "  author    = {Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev},\n",
    "  title     = {Improving Text-to-SQL Evaluation Methodology},\n",
    "  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n",
    "  month     = {July},\n",
    "  year      = {2018},\n",
    "  address   = {Melbourne, Victoria, Australia},\n",
    "  pages     = {351--360},\n",
    "  url       = {http://aclweb.org/anthology/P18-1033},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is a modified version of the example [tagger code](https://github.com/clab/dynet/blob/master/examples/tagger/bilstmtagger.py) from the DyNet repository.\n",
    "\n",
    "It is available under an Apache 2 license."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
